<!DOCTYPE html>
<html lang="en"><head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Jennie Tram Le</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">
<div id="skipnav"><a href="#maincontent">Skip to main content</a></div>

    <!-- Navigation -->
    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top navbar-custom affix-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand" href="#page-top">Jennie Tram Le</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden active">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll">
                        <a href="#about">ABOUT</a>
                    </li>
					
					<li class="page-scroll">
                        <a href="#resume">RESUME</a>
                    </li>
					
                    <li class="page-scroll">
                        <a href="#portfolio">PORTFOLIO</a>
                    </li>

                    <li class="page-scroll">
                        <a href="#social">CONNECT</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container" id="maincontent" tabindex="-1">
            <div class="row">
                <div class="col-lg-12">
                    <img class="img-responsive" id="profilePicture" src="img/profile.png" alt="">
                    <div class="intro-text">
                        <h1 class="name"><br>Hi, It's Jennie!</h1>
                        <!--hr class="star-light"-->

                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>ABOUT</h2>
                    <hr class="star-primary">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-lg-offset-2">
                    <p>2 years of experience in management consulting and information services industries. Currently graduated with a Master in Business Analytics from Gabelli School of Business, Fordham University. Passionate about data visualization, machine learning and big data. Experienced in Python, R, Tableau, SQL and Google Analytics</p>
				</div>
                <div class="col-lg-4">
                    
                    <p>I love to create beautiful visualizations that tell meaningful stories and add values to businesses and society. As a hardcore fan of Rap and classical music, I am also fascinating about engineering new features to build appealing music recommendation systems with machine learning. Whether it's G-eazy or Mozart playing in the background, I'd like to channel the energy and creativity from music into my work and passion projects. In my free time, I enjoy taking visualization challenges and sharing the stories to the Public Tableau Community and other social websites. While I'm not coding or building dashboards, I cooked and invented new recipes with my roommates, played chess with my neighbors, practiced a new piano pieces, or read Malcom Gladwell's books at Madison Square Park.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Resume Section -->
    <section  id="resume">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>RESUME</h2>
                    <hr class="star-primary">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <div class="portfolio-item">
                        <a href="#resumeModal" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <iframe width="100%" height="500px" src="file/Resume.pdf"></iframe>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Portfolio Grid Section -->
    <section id="portfolio">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>PORTFOLIO</h2>
                    <hr class="star-primary">

                </div>
            </div>
            
            <div class="row">
                <div class="col-sm-4 portfolio-item">
                    <a href="#portfolioModal1" class="portfolio-link" data-toggle="modal">
                        <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        <img src="img/bot.png" class="img-responsive crop" alt="Tweeter Projects">
                    </a>
                </div>
		    
               <div class="col-sm-4 portfolio-item">
                    <a href="#portfolioModal2" class="portfolio-link" data-toggle="modal">
                        <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        <img src="img/song.png" class="img-responsive crop" alt="Song Projects">
                    </a>
                </div>
		    
                <div class="col-sm-4 portfolio-item">
                    <a href="#portfolioModal3" class="portfolio-link" data-toggle="modal">
                        <div class="caption">
                            <div class="caption-content">
                                <i class="fa fa-search-plus fa-3x"></i>
                            </div>
                        </div>
                        <img src="img/google.png" class="img-responsive crop" alt="Google Q&A">
                    </a>
                </div>   
                <div class="row">
                        <div class="col-lg-12 text-center">
                            <h2>Tableau Visualization</h2>
                            <hr class="star-primary">
                            <p>For more visualization, visit <a href="https://public.tableau.com/profile/tram.ngoc.le#!/"">my Tableau profile</a>.</p>
		            <div class='tableauPlaceholder' id='viz1605473535909' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;CO&#47;COVID-19NEWCASESTRENDINTHEUS312020-8242020&#47;COVID-19NEWCASESTREND&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='COVID-19NEWCASESTRENDINTHEUS312020-8242020&#47;COVID-19NEWCASESTREND' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;CO&#47;COVID-19NEWCASESTRENDINTHEUS312020-8242020&#47;COVID-19NEWCASESTREND&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1605473535909');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1250px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1250px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='727px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
			    
                     
                        </div>

                </div>
                

            </div>    
        </div>
    </section>



    <!-- Footer -->
    <footer class="text-center">
      <section id="social">
        <div class="footer-above">
            <div class="container">
                <div class="row">
                    <div class="footer-col col-md-4">
                    </div>
                    <div class="footer-col col-md-4">
                        <h3>Connect</h3>
                        <ul class="list-inline">
                            <li>
                                <a href="https://www.linkedin.com/in/jennie-tram-le-6543b8b6/" target="_blank" class="btn-social btn-outline"><span class="sr-only">LinkedIn</span><i class="fa fa-fw fa-linkedin"></i></a>
                            </li>
                            <li>
                                <a href="https://github.com/bigforehead" target="_blank" class="btn-social btn-outline"><span class="sr-only">Github</span><i class="fa fa-fw fa-github"></i></a>
                            </li>
                            <li>
                                <a href="https://www.instagram.com/lengoctram/" target="_blank" class="btn-social btn-outline"><span class="sr-only">Instagram</span><i class="fa fa-fw fa-instagram"></i></a>
                            </li>
                            <li>
                                <a href="mailto:tle35@fordham.edu" target="_blank" class="btn-social btn-outline"><span class="sr-only">Email</span><i class="fa fa-fw fa-google"></i></a>
                            </li>
                        </ul>
                    </div>
                    <div class="footer-col col-md-4">
                    </div>
                </div>
            </div>
        </div>
        <div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                         © 2020 Jennie Tram Le
                    </div>
                </div>
            </div>
        </div>
        </section>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll hidden-sm hidden-xs hidden-lg hidden-md">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>

    <!--------Tweeter Projects--->
    <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                          <h2>Twitter Malicious Bot Detection</h2>
                          <hr class="star-primary">
                          <div class="modal-text">
                            <p>
                                Bots on Twitter are accounts that are controlled by computer programs, automatically producing content, and interacting with other accounts. These programs are turned on and off without following a pattern, making them hard to identify. Some bots only exist for providing some interesting tweets to users daily, but some bots are intentionally spreading disturbing or misleading information, which negatively impacts users’ experiences. Those malicious bots indirectly hurt Twitter’s image and business revenue. Although Twitter has already been able to identify most of the bot accounts, the company could have done better in Malicious Bots Classification. Therefore, this project propose a novel bot type classification method by using TFIDF. With the supervised machine learning method, this study aims to detect malicious Twitter bot types based on bot behaviors and text analysis.
                                <br><br>
                                <b>BIG PICTURE</b><br><br>
                                During the modeling process, the primary source of input data is bots' IDs from Bot Repository. I use Python to crawl users'information and tweets based on these IDs. <br>
				The process is divided into two phase. In phase one, I started off with features extraction with Twitter's metadata then tested multiple supervised machine learning models, such as Decision Tree, Random Forest, and Logistic Regression with IBM SPSS Modeler platform to create testing model with 13 significant indicators to predict behaviors of certain malicious bot types. In phase two, to further improve on the result, I created a dictionary for each type of malicious bots and calculated the TFIDF of keywords in the dictionary of each tweet. Then I again applied the most accurate model from phase one with additional TFIDF features. Finally I got a Random Forest that can predicted malicious bots correctly of 91.68% on the testing set.<br>
                                
				 <img src="img/bot_process.png" class="model-text" alt="Tweeter Projects">
				 
                                <br><br>
                                <b>BASELINE MODEL</b><br><br>
				 <img src="img/modelbaseline.png" class="model-text" alt="Tweeter Projects">
				<br><br>
                                To start off with the baseline model, I first gained interest of using behavior patterns as prediction targets from multiple research papers. The selection includes three major malicious bots: Fake Follower, Scam Bot, and Spam Bot.<br><br>
				The raw data set includes 900 IDs for each kind of malicious bots from Bot Repository, reaching 2,700 IDs in total. From 2,700 IDs, I used Twitter API and Tweepy to crawl twitter's metadata and tweets. Since some accounts have been banned and might not have any tweet, I got 2,042 valid account and crawled 250 tweets for each account. In total, there are 138,042 tweets for Fake Followers, 160,245 tweets for Scam Bots, and 161,956 tweets for Spam Bots.<br><br>
				 <img src="img/botgroupdistribution.png" class="model-text" alt="Tweeter Projects">
				<br><br>
				<b>PHASE ONE - BAD BOT-LIKE BEHAVIOR DETECTION</b><br><br>
				In phase one, typical features of tweet syntax, temporal behavior and user profile are used to build a model that identifies different types of malicious bots. After testing multiple models, I decided to use Random Forest since it results in the highest accuracy rate. For your information, Random Forest is a classification algorithm consisting of many decision tree. I used IBM SPSS to build an uncorrelated forest of trees to predict the type of malicious bots based on 13 identified fields of account's behaviors. The features include user ID for each twitter account, average number of retweets of tweets for each account, average number of times tweets favorited by twitter users for each account, bot type, bot group, number of followers, number of friends, number of tweets for each account, a binary feature of default profile (yes/no), a binary feature of using default profile image, if geo-location is turned on, average number of tweets posted daily per account, and percentage of tweets containing URL or hyperlink for each account.<br><br>
				The target of the Random Forest model is bot group, which containing three nominal values of Fake Followers, Scam Bots, and Spam Bots. The data was partitioned into 70% of the training set and 30% of the testing set. The Random Forest test classified malicious bots with a high level of accuracy, 99.43% correctly on training data, and 91.05% on testing data. Since the training set may be overfitting, I focused on the testing set. Regarding the testing set, the model performed best at predicting Spam Bots, with accuracy of 96.226%, following by Fake Followers and Scam Bots, with accuracy of 89.862% and 87.019%, respectively. Overall, the Random Forests can be applied to classify malicious bot types with the features above at a high level of accuracy. <br><br>
                                The Random Forests Classification applied nine predictor inputs to train 1405 records, which is 70% of the dataset. The result indicated that some features matter more than others. Pertaining malicious bot behaviors classification, the model identified the most important predictor as average of retweet count, following by number of followers, average daily tweet, number of tweets, number of friends, percentage of tweets containing URL or hyperlink per account. Using the default profile, the average number of favorites and turning on geo-location do not demonstrate significant results for classification.<br><br>
				
				<b>PHASE TWO - BEHAVIORAL FEATURES AND TEXT SEMANTICS DETECTION</b><br><br>
				In phase two, I used Python package NLTK to get keywords dictionary and frequency of each keyword for 216,173 tweets. I calculated every term's frequency for each group of malicious bots. I set he threshold for term frequency rate is 0.05%. I picked every term with that is larger than the threshold to create detection dictionaries for fake followers, scam bots, and spam bots. To calculate fitting degree of each tweet with three dictionaries, I used Python and Excel to calculate they keywords' TFIDF in each tweet and see whether these dictionaries contribute to our prediction. Finally, I normalized TFIDF of each tweet to make sure they are under the same weight then calculated the average normalized TFIDF of each account. Wordcloud are used to check the term frequency, and the results of Word Cloud were consistent with the TFIDF calculation.<br><br>
				In short, Fake Followers have the lowest TFIDF score in all three dictionaries and a little better match in Fake Follower dictionary, probably due to their small number of tweets. Scam Bots are in the middle, and Spam Bots have high TFIDF in all three dictionaries and especially in the spam dictionary. Spam Bots do post tons of tweets, and the result indicates that there are some words in the spam dictionary that have discriminative power to distinct Spam Bots from other bots. We added these three average TFIDF as our new features in our detection model and built a new model to see whether these three new features can contribute to our overall accuracy.<br><br>
				    <b> Spam bot's Tweets WordCloud </b><br><br>

				 <img src="img/type1.png" class="model-text" alt="Tweeter Projects">
				 <br><br>
				    <b> Scam bot's Tweets WordCloud</b><br><br>
				 <img src="img/type2.png" class="model-text" alt="Tweeter Projects">
				 <br><br>
				    
				The Random Forest Classification applied 12 inputs as predicting factors to train 1405 records again. The dataset was partitioned into 70% of the training set and 30% of the testing set. Adding three new inputs of average TFIDF scores of Fake Followers, Scam Bots, and Spam Bots, the model produced a new list of important indicators. The top three indicators now are average of retweet count for each tweet, the number of followers, and average TFIDF score of the spam bot. According to the result, the TFIDF score plays a significant role in classifying types of malicious bots. This new model predicted malicious bots correctly of 91.68% on the testing set.<br><br>
				  <img src="img/P2indicator.png" class="model-text" alt="Tweeter Projects">
				    <br>
				   <img src="img/P2indicator2.png" class="model-text" alt="Tweeter Projects">
				 <br><br>
	
                                <b>EVALUATION</b><br><br>
                                In general, the model of phase two performed slightly better than the model of phase one based on the comparison of accuracy. I can’t tell whether it’s due to the new features or the random error. However, one thing I am sure about is that the TFIDF certainly contributes to analyze the bot types as the model could predict correctly of 81.16% on the testing data when I only used these three features to detect different kinds of malicious bots.<br><br>
                                 <img src="img/tweetcompare12.png" class="model-text" alt="Tweeter Projects">
				<br><br>
                                <b>CONCLUSION</b><br><br>
                                In this project, I combined supervised machine learning and natural languague processing to classify different types of malicious bots on Twitter. After intensive research of related works, I found out that even though most of them researched about bot identification, and even though some of them also studied malicious bots identification, none of them use keyword detection.<br><br>
				This project targets on malicious accounts' tweets as a means to build keyword dictionaries. Both detection tools are more and more advanced, but bots are also becoming more human-like, which makes them more complicated to detect. I focus on finding features related to natural language. <br><br>
				In phase two, I trained a Random Forest classifier using the features of average TFIDF score of keywords in each type dictionary. I was able to create a multi-class classifier that performs with 91.68% of accuracy when classifying into one of 3 classes.<br><br>
				Overall, Fake Followers are inactive accounts with the highest score of TFIDF_fake, Scam Bots are also inactive accounts but with high retweet counts and the highest score of TFIDF_scam, and Spam Bots are active accounts with the highest score of TFIDF_spam. Besides, I can predict Spam Bots the best using the TFIDF score since TFIDF_spam is in the top three important indicators of the model. 
                                <br><br>
				 <div class='tableauPlaceholder' id='viz1585549357967' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwitterMaliciousBotDetection&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='TwitterMaliciousBotDetection&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwitterMaliciousBotDetection&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1585549357967');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1024px';vizElement.style.height='795px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1024px';vizElement.style.height='795px';} else { vizElement.style.width='100%';vizElement.style.height='1477px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
                                <b>REFERENCES</b><br>
                                [1] Stefano Cresci et al. The paradigm-shift of social spambots. arXiv:1701.03017v1 [cs.SI] 11 Jan 2017. Available at: https://arxiv.org/abs/1701.03017<br>
				[2] Cresci-2017 and Pronbots-2019 : https://botometer.iuni.iu.edu/bot-repository/datasets.html. Accessed: 2019-11-26.<br>
				[3] Lulwah Ahmad AlKulaib. Twitter Bots Multiclass Classification Using Bot-Like Behavior Features. B.S. in Computer Science, Gulf University for Science and Technology, June 2011.<br>
				[4] Efthimion, Phillip George; Payne, Scott; and Proferes, Nicholas (2018) "Supervised Machine Learning Bot Detection Techniques to Identify Social Twitter Bots," SMU Data Science Review: Vol. 1: No. 2, Article 5. Available at: https://scholar.smu.edu/datasciencereview/vol1/iss2/5.
                                <br><br>
                                <i>Available on <a href="https://github.com/bigforehead/">Github</a>.</i><br><br>
                              <iframe width="100%" height="500px" src="file/twitterbot.pdf#toolbar=0"></iframe><br><br>
                             
				    
                               
                            </p>

                          </div>
                          <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
				    

    <!--------Song Projects--->
    <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                          <h2>A Song Recommender System on Million Song Dataset</h2>
                          <hr class="star-primary">
                          <div class="modal-text">
                            <p>
                       
                                <br><br>
                                <b>BIG PICTURE</b><br>
                      
                                This project aimed to build an useful and comprehensive music recommendation system based on the Million Song Dataset and musiXmatch dataset. In details, the two datasets include data of user listening history, song metadata, artist metadata, artist similarity, and lyrics. To build the recommendation systems, three algorithms were employed, including popularity-based, collaborative filtering, and content-based in order to generate three different recommendation lists - Hot Song List, Personal List, and New User List. Those algorithms were compared and combined to establish recommending strategy for different scenarios and users. ALS algorithm was used for a personal recommendation, and lyric-based and artist-based similarity recommendation was employed to solve the cold-start problem. The whole system was built with Python and PySpark on Google Cloud Platform and AWS.<br><br>
				In this project, we want to focus on the streaming music industry. The potential benefits of a state of art recommendation system are valuable for streaming music industry. Some possible benefits include the improvement of user retention and engagement as well as a comprehensive understanding of customers' taste and the changing trend of their tastes.

                                <br><br>
				<b>DATASET DESCRIPTION</b><br>
				The Million Song Dataset was originally started as a collaborative project between The Echo Nest and LabROSA. The dataset contains metadata, or derived features, for one million songs. The size of the entire data scales is up to 280GB. The Million Song Dataset is also a cluster of seven complementary datasets provided by the community. In this project, besides the subset, Taste Profile and musiXmatch are used to build the music recommendation system.<br><br>
				Regarding the Million Song subset, besides 1,000,000 unique tracks ID data, the song metadata, the links between artist ID and the tags, and artists’ similarity were extracted using the SQLite databases. For artists, the artists’ location data was also used.<br><br>
				The Taste Profile subset contains user listening history. The dataset scales up to 488MB, including 1,019,318 unique users, 384,546 unique MSD songs, and 48,373,586 user-song-play count triplets.<br><br>
				The musiXmatch dataset provides official lyrics collection of the Million Song Dataset in the bag-of-words format as well as the full mapping of MSD IDs to musiXmatch IDs. The mapping comes as a text file, including 779 thousand matches. The lyrics are directly associated with Million Song Dataset tracks regarding similar artists, tags, years, and audio features. We extracted the lyrics with the SQLite database. Each track is described as the word-counts for a dictionary of the top 5,000 words across the set. The split of training and testing was done according to the split for tagging, regarding tagging test artists. There are 210,519 training bag-of-words, 27,143 testing ones. The dataset contains track ID, mxm ID, word of lyrics, and frequency of the words features.<br><br>
				After mapping all the musiXmatch IDs, track ID, and song ID from listening history, the final dataset scaled up to 3GB.
			
				<br><br>
                                <b>BASELINE MODEL</b><br>
				    <img src="img/song_baselinemodel.png" class="model-text" alt="Song Projects">
				<br>
				     
				Data preprocessing is divided into 2 steps: data consolidation between taste profile, one million unique track IDs and MusiXmatch IDs, and table joins. Then information regarding seeds, listening history, track metadata, artists information, artist’s similarity, and lyrics were saved into separate files for further analysis.The data consolidation was divided into two steps: first step is to map song IDs to track IDs, and second step is to map track IDs to musiXmatch ID. The final dataset covered 90% of the songs in the original listening history. Those IDs are saved as seeds, and taste profiles containing user ID, track ID, and frequency as listening seperately. 
                     
				
				<br>
				Next, metadata of tracks were extracted from SQLite database ("track_metadata.db").
				
			
				<br>
				For lyrics data, data were extracted from "lyrics.db" SQLite database.
				
				
				<br>
				For artists similarity data, target artist IDs and similar artist IDs were fetched from "artist_similarity.db" SQLite database. 
				
                                <br><br>
				<b>ALGORITHM DEVELOPMENT</b><br>
				<b>1. Popularity Benchmark</b><br>
				As a benchmark for recommending songs, recommending popular songs is better than random guesses. Data were filtered with total frequency based on listening history then top 10 heated tracks were selected. This would be the baseline saver in case there is no data for a new user (cold-start problem).
				<br><br>
				<b>2. Collaborative Filtering</b><br>
				Collaborative filtering is probably one of the most used approaches for the recommendation system, or at least as a necessary component of some advanced recommendation structures. However, it has a surprisingly simple requirement for data. It needs nothing else but users’ historical preference on a set of items. The main assumption here is that the users (listeners in our case) who have agreed in the past are very likely to agree in the future. For multiple users, it assumes if user A liked the same product as user B, A would also like to have similar taste as B for other B’s favorites.<br>
				In terms of the ratings, it often requires some metrics to indicate user’s preference, which can either be explicitly available data such as 5 stars or thumbs-up on the product, or an implicitly derived score such as clicks, number of purchases or other data recorded in the cookie. In this case, traditional way was implemented by utilizing the listening frequency as the rating. The assumption when lacking explicit rating is that a higher listening frequency equates to a higher rating.<br>
				For ALS, a matrix with frequency of each song played by each user was constructed. Certainly, this matrix is sparse with tons of missing value, as a limited number of users have listened to only a limited set of songs. The high-level idea is to approximate the matrix by factorizing it as the product of two matrices: the user matrix that represents each user, while the item matrix describes properties of each track.<br>
				  <img src="img/ALS_matrix.png" class="model-text" alt="Song Projects"><br>
				For a reasonable result, two matrices were created so that the error for the known user-song pairs is minimized. The error here refers to the rooted mean squared error (RMSE). In a more detailed technical level, ALS would first fill the user matrix with a random value, and then optimize the song matrix value by minimizing the error. After that, it “alters”, which means it would hold the song matrix and optimize the value of the user’s matrix. This step minimizes two loss functions alternatively and should achieve some optima. <br>
				During the implementation, Spark ML pipeline was used with the following settings. A big set of the parameters were picked and grid-search with cross-validation on this space was executed. Some important parameters to tune include: maxIter (the max number of iterations), rank (the number of latent factors, which basically determined the shape of matrix), and regParam (the regularization parameter).
                              
                                <br><br>
				<b>3. Content-Based Filtering</b><br>
				Content-based filtering recommended similar songs based on the cosine similarity of different features of lyrics to a user's profile. Each songs' lyrics were characterized by its (1) TFIDF, (2) Word2vec, (3), Topic Model with LDA. The Features were represented in matrices, and the cosine similarity was calculated between matrices. <br>
				In this case, the content-based filtering was designed for new users who came to search for a specific song, there was no existing user's profile. The input song's name and artist's name were taken as the user's profile and then the specific song's index was matched in the cosine similarity matrix to return the 10 similar songs. <br>
				Furthermore, artists similarity was taken into account. The similarity scores of one specific artist's similar artists' songs for the specific song an user searched for were matched in the cosine similarity matrix, and the top 10 similar songs belong to the similar artists were returned. Finally, along with the specific song the user searched for, the algorithm recommended 20 other songs that were closet in cosine distance to the user's input song.<br>
				Three different cosine similarities were used to recommend 20 songs, respectively, and the overlapping songs among three results were taken as the final recommendations.<br>
				     <img src="img/content_base_algorithm.png" class="model-text" alt="Song Projects"><br>
				TFIDF for Lyrics<br>
				After stopwords removal and Porter stemming for lyrics, the TF-IDF Vectorizer in the python scikit-learn package was used to calculate the TFIDF of each song's lyrics. However, the was a limitation in the dataset as the lyrics came in the form as bag-of-words rather than the original lyrics. The actual lyrics were protected by copyright, and Million Song Dataset did not have permission to redistribute them, therefore, only unigram could be used. <br>
				After getting the TFIDF matrix, the cosine_similarity in the python scikit-learn package was used to calculate the cosine similarity scores among different tracks based on the TFIDF.
				<br><br>
				Word2vec for Lyrics<br>
				Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus, and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand. The purpose and usefulness of Word2vec are to group the vectors of similar words in vector space. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.[2]<br>
				The data size was so large that calculating the cosine similarity based on the high dimensional sparse TFIDF matrix was time-consuming and required a lot of memory. Thus, the Word2vec was used in our second method. However, the same limitation of lyrics made Word2vec less accurate since there was no context as only unigram could be used.<br>
				The Word2Vec model in the python gensim package was used to create vectors for each song’s lyrics. The words appearing only once were ignored, the total dimension of the word vectors was set 200, and the window size was set as 1 because there were only unigrams in the lyrics.<br>
				After getting the Word2vec matrix, the cosine_similarity in the python scikit-learn package was used again to calculate the cosine similarity scores among different tracks based on the vectors.
				
				<br><br>
				LDA for Lyrics<br>
				Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of the topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.[5] Thus, the big idea behind LDA is that each document can be described by a distribution of topics, and each topic can be described by a distribution of words.<br>
				The assumption that there must be similar topics among all songs was used in this case.Limiting the number of topics could also help to save time and memory when dealing with a large dataset. Therefore, the LDA was used in the third method. LDA ignores syntactic information and treats documents as bags of words, so the unigram format in the dataset doesn’t matter too much in this method.<br>
				The LDA Model in the python gensim package was used to create a topic matrix for all songs’ lyrics. Only the top 10 topics were selected in the model.<br>
				After getting the topic model matrix, the cosine_similarity in the python scikit-learn package was used again to calculate the cosine similarity scores among different tracks based on the topics’ weights.<br>
				<br>
				Artist Similarity<br>
				Million Song Dataset offered a dataset containing similarity among artists, which expanded the content of our content-based filtering. This data was used after the cosine similarity was calculated. The similar artists’ songs were into account, even though they were not in the top 10 similar songs list as the assumption was that similar artists would attract the same audiences in some way.
				<br><br>
				After features creation, two key functions were defined to accomplish the recommendation system. The first function was defined to return the top 10 recommended songs simply based on the cosine similarity. The second function was defined to find the other top 10 recommended songs based on the artist similarity combined with cosine similarity. Finally, these two functions would create a DataFrame that contained the user input song along with the other 20 recommended songs. Three DataFrames would be created based on three cosine similarities, and they were merged to return the overlapping songs, which could be the most similar songs to the user input song.
				<br><br>
				<b>RESULT AND EVALUATION</b><br>
				<b>1. Collaborative Filtering</b><br>
				    Here is the benchmark of our top 10 most listened songs. <br>
				    <img src="img/CL_result.png" class="model-text" alt="Song Projects"><br>
				The data was split into three parts: 60% as train, 20% as validation, and 20% as test. Grid search and cross-validation on validation set was implemented to determine the best parameters of the recommendation system and only recommend the results to the new users in the test set once to avoid information leakage. The process was implemented from scratch and was locked in a pipeline. This work was constructed to avoid overfitting problems.<br>
				As for Spark 2.0, when asked to provide a rating for new users never seen before, the ALS could only yield NAN value. . Therefore, it was impossible to adapt Spark ML’s Cross Validator to check the RMSE. So the algorithm was set to drop NAN values by default with customized cross validation process before using RMSE for evaluation.<br>
				The following configuration steps were executed: (1) set appropriate parameters for users, items and rating; (2) fit ALS and transform the table to generate the prediction column; (3) run predictions against the validation set and check the error; (4) finalize the model with the best RMSE score. <br>
				Exploratory analysis showed that more than half of the songs in the sample were listened at least once. Thus, it could be inferred that songs played more than once can better represent users' tastes and run the ALS on two versions of the datasets.<br>
				Result comparison of model with all data and frequency >=2 data was shown as followed: <br>    
				    <img src="img/CL_result_2.png" class="model-text" alt="Song Projects"><br>
				Except for cross-validation, another common measurement is to compare the model with fake test data where every rating is the average number of frequencies from the train data. While the model excluding the 1-time listened songs returns a slightly higher RMSE score on the test dataset, it behaves better when compared with the average frequency. And this may hint the recommendation makes more sense. The highlighted tracks can be of the highest recommendation quality, as they are the overlap between two ALS recommendation models.
				<br><br>
				<b>2. Content-Based Filtering</b><br>
				Let's simulate a new user searches for a song to see the recommendation results.<br>
				    <img src="img/CB_simulate.png" class="model-text" alt="Song Projects"><br>
				Three DataFrames were created after the function was run. The results showed: 4 same songs between TFIDF and Word2vec methods; 2 same songs between TFIDF and LDA methods; 2 same songs between Word2vec and LDA methods; 1 same song among three methods. Therefore, the final recommended song related to the Box Tops’ Soul Deep was Four Tops’ You Keep Running Away.<br>
				    <img src="img/CB_result.png" width="800" height="633" class="model-text" alt="Song Projects"><br>
				<b>LYRICS ANALYSIS</b><br>
				Besides recommendation system building, further exploration of lyrics was conducted by implementing the LDA model and visualizing the most popular 30 words in lyrics based on the time, including songs from the 2000s, 90s, 80s, 70s, 60s, and 50s.<br>
				<br>
				<b>1. Topic Modeling Visualization</b><br>
				Regarding the LDA model in lyrics analysis, it is noted that the topic modeling visualization was slightly different from the result in the previous content-based recommendation system as the lyrics were not processed with stemming and the model defined 20 topics instead of 10 topics. The original word form of lyrics were kept for a more comprehensive lyrics topic demonstration. Therefore, the implementation included removing stopwords, generating token dictionary class, and building a corpus. The corpus and dictionary were filetered into the LDA model with 20 topics. <br>
					<img src="img/LDA_lyrics.png" class="model-text" alt="Song Projects"><br>
				Some interesting results were found from LDA clustering of 20 topics. The interactive illustration of clusters can be reviewed in the Jupyter notebook (posted on my github). The graph has shown that the cluster 5,10,18, and 16 as well as cluster 20,19 and 14 were separated from other clusters in different dimensions. The reason for this separation was the difference of language as those clusters were not English. On the other hand, cluster 11, 17, 12, and 7 contains lyrics with darker meaning or religious meaning. For instance, we could refer cluster 11 to religion as the most relevant words are “world”, “life”, “live”, “god”, “us”, “heaven”, “god”, “jesus”, “soul”, “angle”, and so on. However, topic in cluster 12 could be related to some darker meaning, or rap song, since the lyrics contained very negative words, such as the “f” word, “kill”, “dead”, “hate”, “hell”, “gun”, “shit”, “bitch”, “war”, “sick”, “shot”, or “murder”. Those clusters were placed on the fourth dimension (lower right side) of the graph. The first dimension (upper right side) included topics with more positive vibes, such as love or party. For instance, cluster 15 could be referred to as a “dance party” as the lyrics included mostly “oh”, “ooh”, “ah”, “yeah”, “shake”, “mama”, “yes”, “babi”, “ohh” and so on. There are multiple clusters relating to “love” mixed together since this topic tends to contain multiple emotions. For example, we can refer to cluster 8 as “happy love” as the lyric contains “love”, “want”, “need”, “feel”, “like”, “kiss”, “true”, “touch”, “give” and so on. Cluster 1 and cluster 2 can refer to a “sad love” or “break up” as the lyrics contains “away”, “see”, “day”, “feel”, “dream”, “fall”, “eye”, “heart” , “time”, “never”, “know”, “think”, and so on, which can demonstrate the longing or sad feeling.<br>
				In general,a good class of 20 topics could be defined with the LDA model. Each topic was approximately well-defined with different themes, such as religion, dance party, dark drama, happy love, and sad love. 
				<br><br>
				<b>2. Lyrics Analysis Through The Time</b><br>
				The second analysis of lyrics was conducted relating to the timeline of the songs. The changes of lyrics from songs from the 50s to 2000s were examined. Lyrics data were merged with track metadata. The lyrics were preprocessed with stemming, stopwords removal, and tokenization. Bar charts of the top 30 words with the highest frequency for each period were plotted.<br>
				Some interesting results based on the observation of the top 30 words in lyrics over the time was found. “Love” seems to be the greatest inspiration in songs of the 50s, 60s, 70s, 80s and 90s as the top words repeated over the past more than 50 years. From 2000 to 2011, the word “love” took the second place while the word “know” became the most popular word. In general, the word “love”, “know”, “like”, “got” were always popular from the 50s to the 2000s. <br>
				Lyrics in 2000s: <br>   
				   <img src="img/lyrics_2000s.png" class="model-text" alt="Song Projects"><br>
				Lyrics in 1990s: <br>  
				   <img src="img/lyrics_1990s.png" class="model-text" alt="Song Projects"><br>
				Lyrics in 1980s: <br>  
				   <img src="img/lyrics_1980s.png" class="model-text" alt="Song Projects"><br>
				Lyrics in 1970s: <br>  
				   <img src="img/lyrics_1970s.png" class="model-text" alt="Song Projects"><br>
				Lyrics in 1960s: <br>  
				   <img src="img/lyrics_1960s.png" class="model-text" alt="Song Projects"><br>
				    
				Lyrics in 1950s: <br>  
				   <img src="img/lyrics_1950s.png" class="model-text" alt="Song Projects"><br>
				Comparing the most popular words in lyrics of the 60s and 2000s, the lyrics in the 60s tended to bring a more subtle vibe than the 2000s did. The 60s lyrics had “babi”, “know”, “love”, “yeah”, “oh”, and “got”, which demonstrated the priority in love and the lover. At the same time, the 2000s lyrics had “love”, “la”, “know”, “got”, “de”, and “come”, which portrayed a bustling vibe and prioritized love and party at the same time. <br>
				Word Cloud for Lyrics in 1960s: <br>  
				    <img src="img/WC_1960s.png" class="model-text" alt="Song Projects"><br>
				Word Cloud for Lyrics in 2000s: <br>
				  <img src="img/WC_2000s.png" class="model-text" alt="Song Projects"><br>
                                <b>CONCLUSION</b><br>
                                In this project, recommendation system was built using a dataset with 1,019,318 unique users and 384,546 unique songs.ALS algorithm, which combines user and item knowledge, was used for collaborative filtering recommender. This recommender can be applied to old users with sufficient listening history to generate personalized recommendations. Several aspects of song knowledge were combined, including artist similarity, TF-IDF, Word2vec and LDA modeling for lyrics, to build a content-based recommender. The content-based recommender is for new users with only one or a few searches and listening history. Similar songs for the current song will be recommended. Considering that Spotify has about 2 million monthly active users, our project is close to the monthly magnitude of the industry-level. Yet a lot of obstacles during the implementation process still emerged.
				<br>
				There are several lessons can be learned when working with Cloud and Pyspark. First, spark has a high dependency on memory usage. During the testing phase, our virtual instance with 30GB memory crushes from time to time. It would save some cost to choose a high memory specialized instance on google cloud. To run cross validation on the 3GB listening history data, it would be secure to choose the following configuration. It is very helpful to use the Unix command “free -m” to check available memory in time. In addition, we learned that we need to cache the datasets whenever they are likely to be used more than once: <br>
				   <img src="img/lesson_learn_1.png" class="model-text" alt="Song Projects"><br>
				   <img src="img/lesson_learn_2.png" class="model-text" alt="Song Projects"><br> 
				   <img src="img/lesson_learn_3.png" class="model-text" alt="Song Projects"><br>
				However, overcoming memory issues on cloud alone is not sufficient. Some default spark settings needed changing to permit more executing memory and driver memory resources to be adapted by Spark. <br>
				In addition, debugging with spark is quite suffering, as the error message is not informative at all. A common mistake is that the data type is not fixed or switched during the processing. Due to the Java development environment, different methods are very strict on input data type. Casting the data types should be a regular task while working with Spark. For example, ALS function in spark only allows integer input. It would return error even when the data type is big integer and so on. In the same vein, the raw ideas from the original data frame are no longer useful that new integer id pairs have to be generated. <br>
                                An example of change the Data Type to Int:<br>
				    <img src="img/lesson_learn_4.png" class="model-text" alt="Song Projects"><br>
				An example of change ID's Data Type from Bigint to Int:<br>
				  <img src="img/lesson_learn_5.png" class="model-text" alt="Song Projects"><br>
				Curently, there are still obstacles towards a final hybrid recommendation system. At this stage, the main difficulty comes from the lack of valid lyrics data. As for further steps in the future, combining the collaborative filtering and content-based recommendation system can provide hybrid recommendations. According complementary advantages from various recommendation algorithm, hybrid solution such as using overlappings would provide users with suggestions of higher quality.For example, a hybrid system based on lyrics and listening history could make sure the users not only like the explicit topic of the track but also like the genre from a taste perspective.<br>
				As a matter of fact, more dimensions of recommendation are always better. Google naturally combined plenty of recommendation strategies in its wide and deep recommendation system with neural networks and ensemble methods. Though it is difficult to bring this project to such a top level, but this precious practive undoubtedly helped lay a solid foundation for potential industrial workds involving recommendation systems or distributed systems.<br>
				Besides, there are some new ideas for further exploration: <br>
				    1. User2Vec: <br>
				    Word2Vec for NLP was used in this project to analyze the knowledge about songs. What if songs are considered as words and users are considered as documents ? The listening history of a user is the text content of the document. Then Word2Vec and Doc2vec can be apply to find similar songs and users.<br>
				    2. Graph algorithm: <br>
				    Graph database is another new trend for online shopping recommendation systems. When modeling songs, artists, and users in a graph database, graph algorithms can help to find the relationship among those features and consider knowledge about songs, artists, and users at the same time. <br>
				    3. Content-based filtering using music audio:<Br>
				    In this project and most current business implementation, content-based filtering in the music industry means lyric-based or text-based. However, machine learning and and deep learning are also having great processes in audio processing. Analyzing the music audio directly could be a new direction for content-based filtering.
				    
				<br><br>
                                <b>REFERENCE</b><br>
                                1. A Beginner's Guide to Word2Vec and Neural Word Embeddings. (n.d.). Retrieved from https://pathmind.com/wiki/word2vec<br>
				2. Content-based Filtering. (2012, January 24). Retrieved from http://recommender-systems.org/content-based-filtering/<br>
				3. Karantyagi. (n.d.). karantyagi/Restaurant-Recommendations-with-Yelp. Retrieved from https://github.com/karantyagi/Restaurant-Recommendations-with-Yelp<br>
				4. Li, S. (2018, June 1). Topic Modeling and Latent Dirichlet Allocation (LDA) in Python. Retrieved from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24<br>
				5. 5. MODELING. (n.d.). Retrieved from https://xindizhao19931.wixsite.com/spotify2/modeling <br>
				6. Welcome! (n.d.). Retrieved from http://millionsongdataset.com/
                                <br><br>
				 
			      <i>Available on <a href="https://github.com/bigforehead/Big-Data-Million-Song-Music-Recommendation-System">Github</a>.</i><br><br>
                              <iframe width="100%" height="500px" src="file/recommendation.pdf#toolbar=0"></iframe><br><br>
                                
                    
                        
                               
                            </p>

                          </div>
                          <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-------- Google Q&A--->
    <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                          <h2>Google Q&A Topics & Types Classification</h2>
                          <hr class="star-primary">
                          <div class="modal-text">
                         
                                <br><br>
                                
				<b>BIG PICTURE</b><br>
				 Computers have evolved quickly nowadays and focused more on developing an algorithm to answer a question like a human. Google Search has become the domain search engine developer in the United States market since 2009. Recently, Google Research team, dedicated to advancing natural language processing and other types of machine learning, has gathered over 6k records of question-answer paired data to build predictive classification models that can perform the more human-like subjective question-answering algorithm. <br>
				 Building a subjective and better human-like intelligent question and answer (Q&A) system is a cutting-edge problematic topic among data scientists, especially when combining text analytics and machine learning techniques. By improving the Q&A system, every time a customer searches on Google, the search engine will retrieve a much more precise answer that meets a desirable answer for an inquiry. The approach of building content predictive classification algorithms and type classification algorithms, and then evaluating these algorithms may demonstrate insight into that topic. <br>
				 This project aimed to build and evaluate classification models with different target measurements, including content classification models for the content of question and answer based on categories and type classification models for the probability of answer and question based on their types. <br>
				 Results of predictive classification models showed that Support Vector Machine model with feature Term Frequency – Inverse Document Frequency (TF-IDF) in the Q&A contents classification had the highest prediction accuracy in both question and answer; while Support Vector Machine model with feature TF-IDF in the type classification had the highest prediction accuracy in question, and Long Short-Term Memory model with feature TF-IDF in the Q&A types classification had the highest prediction accuracy in answer. <br>
			
                                <br><br>
				<b>DATA DESCRIPTION</b><br>
				 The Google QUEST Q&A Labeling data set is featured on Kaggle, the world’s largest data science community. Google Research team, CrowdSource, collected the data from various Stack Exchange properties. Google used this data set to hold a public featured code competition for Kaggle users. The competition aims to improve automated understanding of complex question and answer content. This data set contains approximately 6500 pairs of question and answer gathered from 70 websites.<br>
				 The dataset includes a training dataset and a testing dataset. Both datasets include questions id (qa_id), question_title, question_body, question_user_name, question_user_page, answers, answer_user_name, answer_user_page, url, host and categories. The training dataset also includes values of 30 labels for each question-answer pair of the question types and answer types, which need to be further processed in preprocessing steps for our models, except the questions, answers and categories.The training data contains some duplicated questions with different answers. The values of 30 labels have continuous values in the range 0 to 1.<br>
				<br>    
				<b>METHODOLOGY DIAGRAM</b><br>
				 <img src="img/google_methodology.png" class="model-text" alt="Google Q&A"><br>  
				<br><br>
				    
				 <b>1. Data Preparation | Classes Labeling</b><br>
				 I manually labeled the target for question type and answer type based on the probabilities score provided by the dataset. The dataset includes values of labels for each question-answer pair of the question types and answer types. To make the dataset applicable to classification, I processed the dataset and gave each Q&A pair two new features based on the values, which were question target and answer target. I introduced the two new features using the following rules: the label of a question with the largest value will be the question target; and if there are more than 2 largest values, the question target will be labeled as “mixed”. The same rule is applied to answer targets. The following figures provide an example.<br>
				  <img src="img/google_label.png" class="model-text" alt="Google Q&A"><br>  
				 I assigned id for each category, answer type and question type based on the target.<br><br>
				 <b>2. Variables Selection</b><br>
				 In part one, Q&A category classification, I assigned “category” as our target. “Question_body” and “answer” were selected as the inputs for feature extraction and feature engineering.<br>
				 Based on initial descriptive statistical analysis, the categories of question and answer pairs were skewed towards Technology and Stackoverflow category, which contained 40% and 21% of the dataset’s observations, respectively. Therefore, the problem of imbalanced classes emerged in this dataset. When encountering such problem, there are absolutely difficulties to classification with standard algorithm as the model can be biased towards the majority class.<br>
				    <img src="img/google_category_1.png" class="model-text" alt="Google Q&A"><br>
				 In part two, Q&A type classification, I assigned “target_answer_mix” and “answer_target_mix”, which I generated with the highest probabilities score from each question type features and answer type features from the train data set Kaggle provides. Next, I considered question_body and answer as the input for feature extraction and feature engineering in order to build predictive models.<br>
				 The initial descriptive analysis also shows that there were extremely unbalanced classes in this data set. Regarding question type target distribution of observations, most observations were remained in instruction type, mixed type, choice type, and reason explanation type among 10 classes. Regarding answer type target distribution of observations, answer type procedure was extremely unbalanced towards other classes as this class provided the lowest number of observations.<br> 
				 Question: <br>
				    <img src="img/google_category_2q.png" class="model-text" alt="Google Q&A"><br>
				 Answer: <br>
                            	    <img src="img/google_category_2a.png" class="model-text" alt="Google Q&A"><br>
				 <br>
				 <b>3. Text Preprocessing</b><br>
				 I started off with preprocessing the question body and answer corpuses in preparation of the feature engineering process. I used the nltk package to perform stop-words removal, stemming and lemmatization, punctuation filtering and lowercase. I did not remove all punctuations because some of those might be suitable for the context of technology, stack overflow, or science in category classification part as well as the the structure of question and answer type in types classification part. 
                                 <br><br>
				<b>PART I: Category/Topic Classification</b><br>
				 For feature engineering, I extracted three features, including TFIDF, Word2Vec, and Bag-of-Word (BOW), seperately from the question body and answer corpuses.<br>
				    <1> TFIDF: I extracted TFIDF vectors from the corpus. For TFIDF engineering, I normalized TFIDF term frequency with logarithmic form, minimized the numbers of document a word must be present to be kept to 1, used Euclidean norm of 1 for all features vector, smoothed inversed document frequency weights to prevent zero divisions, and considered both unigram and bigram.<br>
				    The demo of top 5 correlated keywords for unigram and bigram, using TFIDF vectors, regarding each topic was demonstrated as follow:<br>
				     <img src="img/google_demo_1.png" class="model-text" alt="Google Q&A"><br>
				    <2> Bag-of-Word (BOW): I performed tokenization and counted frequency of each token. I set the minimum of document frequency to 1 and only use unigram in this case.<br>
				    <3> Word2Vec: I performed tokenization and built the Word2Vec to vectorize the question and answer body. <br>
				 Through trials of fitting the models with each of the features, TFIDF performed the best among the three features. Bag-of-words might perform well with Naïve Bayes but the accuracy rate was lower than TFIDF with Support Vector Machine. For further details, review the Appendix section in the jupyter notebook. In conclusion, I selected TFIDF as the main feature.<br>
				 Due to the unbalanced classes problem in the data set, I decided to apply Synthetic Minority Oversampling Technique (SMOTE) to randomly oversample the training data set with TFIDF features.<br>
				 <br>
				 <b>Model Building</b><br>
				 Different algorithms were applied to classify 5 topics of culture, science, technology, life arts and stackoverflow. Those models are Naïve Bayes, Support Vector Machine, Decision Tree and Random Forest.<br><br>
				 <b>Model Selection</b><br>
				     <img src="img/google_modelselection_1.png" class="model-text" alt="Google Q&A"><br>
				 I performed performed 5-fold cross-validation on the training dataset and evaluate the model selection based on accuracy rate of the training data set. Support Vector Machine performed the best among all other models. It provided 98% of accuracy for question and 95% accuracy for answer on topic classification. Next, I fitted Support Vector Machine on the testing data set and evaluate the model.<br><br>
				 <b>Model Evaluation</b><br>
				 I used precision score, recall score and F-1 score to evaluate the models.<br>
				      <img src="img/google_evaluation_1.png" class="model-text" alt="Google Q&A"><br>
				 For Question category/topic classification, Support Vector Machine performed on testing data with 0.75 for precision score and recall score and 0.71 for F1-score. Regarding each topic, the precision score ranged from 63% to 80%, which indicated a good performance. Out of all questions that were labeled as culture, technology, stackoverflow, science and life arts, the model predicted, respectively, 80%, 75%, 75%,78% and 63% of them correctly.<br>
				 For Answer category/topic classification, Support Vector Machine performed on testing data with 0.74 for all precision, recall and F-1 score. Those scores were slightly lower compared to that of the question. In general, regarding each class, out of all questions that were labeled as culture, science, stackoverflow, technology, and life arts, the model predicted, respectively 88%, 81%, 63%, 75%, and 68% of them correctly.<br>
				     <img src="img/google_matrix_1.png" class="model-text" alt="Google Q&A"><br>
				 Looking at the confusion matrices, the number of technology questions were the highest correct prediction. However, at the same time, the number of observations labeled as technology was the highest, therefore, we need to look at each class to consider the performance of the model. In general, the model performed prediction the most accurately on questions and answers labeled as culture. On the other hand, the model might confuse between stackoverflow and technology since those topics contained similar keywords relating to code or software.<br><br>
				 <b>PART II: Q&A TYPE CLASSIFICATION</b><br>  
				 In the second part of this project, I classified 10 types of question and 4 types of answers. I evaluated different algorithm including Support Vector Machine, Naïve Bayes, Long Short-Term Memory Recurrent Neural Network, and Artificial Neural Network to decide the best models for question and answer type classification. I seperated the first three models using Python with the final model using SPSS Modeler. Besides, I only used training data set of Kaggle for this part. I partitioned training and testing into 80-20 ratio.
				 For feature engineering, I extracted 2 types of features, 1000 first word counts and TFIDF vectors. After trials, TFIDF still perform better. Therefore, I applied TFIDF features of part one to part two. I normalized TFIDF term frequency and considered both unigram and bigram. <br>
				 Here are some examples of the top correlated keywords as unigram and bigram of TFDF features. <br>
				 Question: <br>   
				    <img src="img/google_demo_2q.png" class="model-text" alt="Google Q&A"><br>
				 Answer: <br>
				    <img src="img/google_demo_2a.png" class="model-text" alt="Google Q&A"><br>
				 <br>
				 <b>Model Building</b><br>
				 I chose chose Support Vector Machine, Naïve Bayes, and Long Short-Term Memory. For Long Short-Term Memory, I performed one hot encoding on the target of question and answer, set weight to the model to deal with imbalanced classes, train for two times and set validation dataset as 20% of the training data. The class weight was calculated based on the proportion of observations of each class against the whole data set. <br><br>
				 <b>Model Selection</b><br>
				 For question type classification, even though Support Vector Machine performed 89.19% accurately on training data, which was slightly lower than LSTM did of 91.2%, Support Vector Machine performed the best on training data with the highest accuracy rate of 50.82%. On the other hand, for answer type classification, LSTM remained as the best classifier with the highest accuracy on training and testing data, 96.9% and 63.89%, respectively. <br>
				 Therefore, Support Vector Machine was chosen for classify 10 types of questions while Long Short-Term Memory was chosen to classify 4 types of answers.<br><br>
				 <b>Model Evaluation</b><br>
				 In order to evaluate the model, I fitted each model on testing data and used precision, recall and f1-score to evaluate the performance of each models.<br>
				 For Question Type classification, Support Vector Machine predicted on an average of 0.49 precision score, 0.51 recall score and 0.49 F1-score. The model predicted on question type instruction with 0.62 of precision score, 0.78 of recall score and 0.69 F1-score. Of all the questions that were labeled as instructions, the model predicted 62% correctly out of them.<br>
				 For Answer Type classification, Long Short-Term Memory predicted on an average of 0.53 precision score, 0.61 of recall score and 0.56 of harmony score. However, the model predicted the best on answer type instructions and answer type reason explanation while poorly predicted answer type mixed and answer type procedure. Of all the answers that were labeled as instruction and reason explanation, the model predicted 66% and 62% correctly, respectively. However, the model could not predict any answer type procedure as all the three scores are 0.<br>
				     <img src="img/google_evaluation_2.png" class="model-text" alt="Google Q&A"><br>
				     <img src="img/google_matrix_2.png" class="model-text" alt="Google Q&A"><br>
				 Looking at the confusion matrices of both question type and answer type predictions, both models did not perform very well on the testing data set. The model predicted the most with question type instruction as well answer reason explanation and answer type instruction. <br>
				 On the question type classification, the model tended to miss classify question mixed type with question type explanation. On the other hand, LSTM tended to miss classify answer type instruction with answer type reason explanation and answer type mixed.<br><br>
				  <b>RESULTS & DISCUSSION</b><br>  
				 Ideally, I would like to build models that can predict both the content and type of question and answer in this project. <br>
				 In topic, or category, classification part, Support Vector Machine with TFIDF performed the best with an accuracy of 98% on training and a precision of 75% on testing for question, and an accuracy of 95% on training data and a precision of 74% on testing data. The performance of part one is great, especially on Culture topic.<br>
				 In Q&A type classification part, Support Vector Machine performed the best for Question type classification with 89% on training yet 50% on testing. Long Short-Term Memory performed the best for answer type with about 91% on training data and 63% on testing data. However, the precision score for both question type and answer type classification on testing data ranged from 50% to 55%. <br>
				 There are two main reasons why models cannot perform well in the second task. Firstly, TFIDF is not a good feature for distinguishing the type of a context. Secondly, there are too many types of the question-answer pair compared with the sample size. The testing data set does not provide enough observations for each type. Also, some of the types have few numbers of samples, which confuses the models. Oversampling would not help in this case since the data will be repeated generated based on only few observations. However, since the target has 10 different groups, the baseline accuracy can be considered as 10% in a random guess. In this case, an accuracy of 50% - 60% is still acceptable in this project. <br>
				 <br>
				   <b>RECOMMENDATIONS</b><br> 
				 For future improvement, feature engineering can be improved with other features for distinguishing the type of a context rather than TFIDF. For instance, new features such as POS (part of speech) can be selected. This feature provides tag for nouns, adjective, adverb and other tags that relating to the structure of a sentence. The structure of a sentence can probably be a better identification of types of question and answer. <br>
				 Besides, combining some of the groups to balance the size among them may also improve the performance. <br>
				 Finally, collecting more data on each type and retrain the model can also help to improve the accuracy.
				 <br><br>
				   <b>CONCLUSION</b><br>
				 In response to the Google question and answer classification project, I have built a model to classify topics of question and answer, Support Vector Machine with TFIDF features. However, for question and answer types classification, the Support Vector Machine and Long Short-Term Memory should be retrained again with different features and more data. Our model for content classification can contribute to the Q&A systems research in an effort of making the system become more human-like.
				 <br><br>
				   
				 <b>REFERENCE</b><br>
				 1. Li, Susan. “Multi-Class Text Classification with Scikit-Learn.” Medium, Towards Data Science, 20 Feb. 2018, towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f.<br>
				 2. Javaid, Nabi. “Machine Learning Multiclass Classification with imbalanced data set.” Medium, Towards Data Science, Dec 22, 2018, towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a.<br>
				 3. “A Gentle Introduction to Imbalanced.” Machine Learning Mastery, Jan. 14, 2020, machinelearningmastery.com/what-is-imbalanced-classification.<br>
				 4. “Best Algorithm for Classification.” Kaggle, kaggle.com/getting-started/42409. <br>
				 5. Mohammed Terry-jack, “Tips and Tricks for Multi-class Classification.” Medium, Towards Data Science, Apr. 28, 2019, medium.com/@b.terryjack/tips-and-tricks-for-multi-class-classification-c184ae1c8ffc.
				 <br><br>
				    
				<i>Available on <a href="https://github.com/bigforehead/Text-Analytics--Google-Quest-Q-A-Multi-Classification">Github</a>.</i><br><br>
                                <iframe width="100%" height="500px" src="file/GoogleQ&A.pdf#toolbar=0"></iframe>

                            </p>

                          </div>
                          <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    

    <!--Resume Modal-->
    <div class="portfolio-modal modal fade" id="resumeModal" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                          <div class="row">
                            <iframe width="100%" height="1000px" src="file/Resume.pdf"></iframe>

                          </div>

                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
	

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/freelancer.js"></script>

    
    
</body></html>
